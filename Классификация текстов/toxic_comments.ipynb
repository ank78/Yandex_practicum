{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Описание-проекта\" data-toc-modified-id=\"Описание-проекта-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Описание проекта</a></span></li><li><span><a href=\"#Преамбула\" data-toc-modified-id=\"Преамбула-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Преамбула</a></span></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Вывод-по-разделу\" data-toc-modified-id=\"Вывод-по-разделу-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Вывод по разделу</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Модель-DummyClassifier\" data-toc-modified-id=\"Модель-DummyClassifier-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Модель DummyClassifier</a></span></li><li><span><a href=\"#Модель-LogisticRegression\" data-toc-modified-id=\"Модель-LogisticRegression-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Модель LogisticRegression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Без-балансировки\" data-toc-modified-id=\"Без-балансировки-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Без балансировки</a></span></li><li><span><a href=\"#Балансировка-с-увеличением-классов\" data-toc-modified-id=\"Балансировка-с-увеличением-классов-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Балансировка с увеличением классов</a></span></li><li><span><a href=\"#Балансировка-с-уменьшением-классов\" data-toc-modified-id=\"Балансировка-с-уменьшением-классов-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Балансировка с уменьшением классов</a></span></li></ul></li><li><span><a href=\"#Модель-LGBMClassifier\" data-toc-modified-id=\"Модель-LGBMClassifier-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Модель LGBMClassifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Без-балансировки\" data-toc-modified-id=\"Без-балансировки-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Без балансировки</a></span></li><li><span><a href=\"#is_unbalance=True\" data-toc-modified-id=\"is_unbalance=True-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>is_unbalance=True</a></span></li><li><span><a href=\"#Балансировка-с-увеличением-классов\" data-toc-modified-id=\"Балансировка-с-увеличением-классов-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Балансировка с увеличением классов</a></span></li><li><span><a href=\"#Балансировка-с-уменьшением-классов\" data-toc-modified-id=\"Балансировка-с-уменьшением-классов-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>Балансировка с уменьшением классов</a></span></li><li><span><a href=\"#Результаты-обучения:\" data-toc-modified-id=\"Результаты-обучения:-4.3.5\"><span class=\"toc-item-num\">4.3.5&nbsp;&nbsp;</span>Результаты обучения:</a></span></li></ul></li><li><span><a href=\"#Вывод--по-разделу\" data-toc-modified-id=\"Вывод--по-разделу-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Вывод  по разделу</a></span></li></ul></li><li><span><a href=\"#Тестирование\" data-toc-modified-id=\"Тестирование-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Тестирование</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "**Цель:** Обучить модель классифицировать комментарии на позитивные и негативные.\n",
    "\n",
    "**Метрика качества:** $\\text{F1} >= 0.75$. \n",
    "\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преамбула\n",
    "\n",
    "<font color='red'>**Важно!!!**</font>\n",
    "\n",
    "\n",
    "Обучение моделей - это мучительно-длительный процесс. Поэтому в рамках проекта обучение проводилось один раз, затем тип ячеек с кодом обучения менялся на RAW, а найденные лучшие гиперпараметры записывались в виде словаря в ячейке ниже, там же выводились результаты после обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn==0.8.0 in /opt/conda/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn==0.8.0) (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn==0.8.0) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn==0.8.0) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.9/site-packages (from imbalanced-learn==0.8.0) (1.21.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.24->imbalanced-learn==0.8.0) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score, roc_curve, roc_auc_score, recall_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "import spacy\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 100\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk_stopwords.words(\"english\"))\n",
    "\n",
    "cpu_use = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/datasets/toxic_comments.csv'\n",
    "try:\n",
    "    df = pd.read_csv(p)\n",
    "except:\n",
    "    df = pd.read_csv(p[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При сохранении датасета кроме данных выгрузили индексы в первой (или нулевой, если брать индексацию питона, а не \"человеческую\") колонке. Они нам не нужны - удалим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Колонки ДО удаления: ['Unnamed: 0', 'text', 'toxic']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Колонки ПОСЛЕ удаления: ['text', 'toxic']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f'Колонки ДО удаления: {df.columns.to_list()}')\n",
    "\n",
    "df = df.drop(columns=df.columns[0])\n",
    "\n",
    "display(f'Колонки ПОСЛЕ удаления: {df.columns.to_list()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача на бинарную классификацию, проверим что признак `toxic` имеет только два значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признак `toxic`, тип `int`, 2 значения - `0` и `1`.  \n",
    "\n",
    "Наша задача: найти токсичные комментарии, при этом если позитивный комментарий ошибочно отправится на модерацию и  \"чуть\" позже появится на странице - небольшая беда и это намного лучше, чем если токсичный комментарий попадет на страницу описания товаров. \n",
    "\n",
    "Для выполнения задачи нам необходимо текстовые данные сначала токенезировать/лемматизироватью В рамках этой задачи мы испытаем две разные библиотеки `nltk` (лемматизация с учетом позиции слова в предложении и без этого)  и `spacy` (лемматизация с учетом позиции слова всего корпуса и лемматизация с учетом позиции слова в конвейере батчами) измерив время  работы каждого способа. После обучения модели LigistciRegression мы проверим качество на каждом варианте лемматизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем разные библиотеки и подходы к лемматизации текстов:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize without POS Tag\n",
    "def lemmatize(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_out = ' '.join([lemmatizer.lemmatize(word) for word in word_list])\n",
    "    clear = re.sub('[^a-zA-Z]', ' ', lemmatized_out)\n",
    "    clear = ' '.join(clear.split())\n",
    "    clear = clear.lower()\n",
    "    if not clear.strip():\n",
    "        clear = 'empty'\n",
    "    return clear\n",
    "\n",
    "df['lemma_no_POS'] = df['text'].progress_apply(lemmatize) \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "df['lemma_with_POS'] = df['text'].str.lower().progress_apply(lambda x: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Lemmatize with POS Tag\n",
    "def lemmatize_spacy(text):\n",
    "    return \" \".join([token.lemma_ for token in nlp(text)])\n",
    "\n",
    "df['lemma_spacy_with_POS'] = df['text'].progress_apply(lemmatize_spacy)  \n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Lemmatize with POS Tag in pipe with batch\n",
    "new_corpus = []\n",
    "\n",
    "for doc in tqdm(nlp.pipe(df['text'], batch_size=64, n_process=-1, disable=[\"parser\", \"ner\"]), total=len(df['text'])):\n",
    "    word_list = [tok.lemma_ for tok in doc]\n",
    "    new_corpus.append(' '.join(word_list))\n",
    "\n",
    "df['lemma_spacy_pipe_with_POS'] = new_corpus\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# Lemmatize without POS Tag in pipe with batch\n",
    "new_corpus = []\n",
    "\n",
    "for doc in tqdm(nlp.pipe(df['text'], batch_size=64, n_process=-1, disable=[\"tagger\", \"parser\", \"ner\"]), total=len(df['text'])):\n",
    "    word_list = [tok.lemma_ for tok in doc]\n",
    "    new_corpus.append(' '.join(word_list))\n",
    "\n",
    "df['lemma_spacy_pipe_no_POS'] = new_corpus\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "df.to_csv('datasets/toxic_comments_lemmas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Время, затраченное на лемматизацию с использованием библиотеки/метода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>library</th>\n",
       "      <th>method</th>\n",
       "      <th>time</th>\n",
       "      <th>speed</th>\n",
       "      <th>remark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nltk</td>\n",
       "      <td>lemma_no_POS</td>\n",
       "      <td>00:03:37</td>\n",
       "      <td>733.11it/s</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spacy</td>\n",
       "      <td>lemma_spacy_pipe_no_POS</td>\n",
       "      <td>00:12:38</td>\n",
       "      <td>210.01it/s</td>\n",
       "      <td>nlp.pipe( batch_size=64, n_process=-1, disable=[\"tagger\", \"parser\", \"ner\"])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spacy</td>\n",
       "      <td>lemma_spacy_pipe_with_POS</td>\n",
       "      <td>00:23:22</td>\n",
       "      <td>113.62it/s</td>\n",
       "      <td>nlp.pipe(batch_size=64, n_process=-1, disable=[\"parser\", \"ner\"])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nltk</td>\n",
       "      <td>lemma_with_POS</td>\n",
       "      <td>00:29:17</td>\n",
       "      <td>90.63it/s</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spacy</td>\n",
       "      <td>lemma_spacy_with_POS</td>\n",
       "      <td>01:03:29</td>\n",
       "      <td>41.81it/s</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  library                     method      time       speed  \\\n",
       "0    nltk               lemma_no_POS  00:03:37  733.11it/s   \n",
       "4   spacy    lemma_spacy_pipe_no_POS  00:12:38  210.01it/s   \n",
       "3   spacy  lemma_spacy_pipe_with_POS  00:23:22  113.62it/s   \n",
       "1    nltk             lemma_with_POS  00:29:17   90.63it/s   \n",
       "2   spacy       lemma_spacy_with_POS  01:03:29   41.81it/s   \n",
       "\n",
       "                                                                        remark  \n",
       "0                                                                               \n",
       "4  nlp.pipe( batch_size=64, n_process=-1, disable=[\"tagger\", \"parser\", \"ner\"])  \n",
       "3             nlp.pipe(batch_size=64, n_process=-1, disable=[\"parser\", \"ner\"])  \n",
       "1                                                                               \n",
       "2                                                                               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_time_lemma = pd.DataFrame({'library': ['nltk', 'nltk', 'spacy', 'spacy', 'spacy'],\n",
    "                              'method': ['lemma_no_POS', 'lemma_with_POS', 'lemma_spacy_with_POS', 'lemma_spacy_pipe_with_POS', \n",
    "                                         'lemma_spacy_pipe_no_POS'],\n",
    "                              'time': ['0:03:37', '0:29:17', '1:03:29', '0:23:22', '0:12:38'],\n",
    "                              'speed': ['733.11it/s', '90.63it/s', '41.81it/s', '113.62it/s', '210.01it/s'],\n",
    "                              'remark': ['', '', '', 'nlp.pipe(batch_size=64, n_process=-1, disable=[\"parser\", \"ner\"])', \n",
    "                                         'nlp.pipe( batch_size=64, n_process=-1, disable=[\"tagger\", \"parser\", \"ner\"])']\n",
    "                             })\n",
    "df_time_lemma['time'] = pd.to_datetime(df_time_lemma['time']).dt.time\n",
    "with pd.option_context('max_colwidth', None):\n",
    "    display(df_time_lemma.sort_values(by='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использование `nltk` отказ от POS (теггирование, для определения части речи) позволяет ускориться на ~ в 10 раз - этот способ самый быстрый из всех.\n",
    "\n",
    "При использование `spacy` конвейер ускорил процесс всего в ~3 раза, в случае отказа от POS между вариантами с конвейером ускорение  в ~2 раза.\n",
    "\n",
    "Разница по времени между`spacy с POS в конвейере` и `nltk с POS` не существенная.\n",
    "\n",
    "Сравним качество лемматизаторов взяв \"из коробки\" LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка файла с леммами\n"
     ]
    }
   ],
   "source": [
    "lemma_file = 'datasets/toxic_comments_lemmas.csv'\n",
    "\n",
    "if os.path.exists(lemma_file):\n",
    "    print('Загрузка файла с леммами')\n",
    "    df = pd.read_csv(lemma_file)\n",
    "else:\n",
    "    df['lemma'] = df['text'].str.lower().progress_apply(lambda x: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) \n",
    "                                                                            for w in nltk.word_tokenize(x)]))\n",
    "    # Сохраним датасет в файл, что бы повторно не лематизировать, а загружать датасет из файла\n",
    "    df.to_csv('datasets/toxic_comments_lemmas.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: lemma_no_POS\n",
      "Column: lemma_with_POS\n",
      "Column: lemma_spacy_with_POS\n",
      "Column: lemma_spacy_pipe_with_POS\n",
      "Column: lemma_spacy_pipe_no_POS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>col</th>\n",
       "      <th>time lemmatize</th>\n",
       "      <th>f1</th>\n",
       "      <th>tpr</th>\n",
       "      <th>count_toxic</th>\n",
       "      <th>detection_count_toxic</th>\n",
       "      <th>diff_with_best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>lemma_spacy_with_POS</td>\n",
       "      <td>1:03:29</td>\n",
       "      <td>0.710763</td>\n",
       "      <td>0.578764</td>\n",
       "      <td>16186</td>\n",
       "      <td>9368.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>lemma_spacy_pipe_with_POS</td>\n",
       "      <td>0:23:22</td>\n",
       "      <td>0.710763</td>\n",
       "      <td>0.578764</td>\n",
       "      <td>16186</td>\n",
       "      <td>9368.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>lemma_with_POS</td>\n",
       "      <td>0:29:17</td>\n",
       "      <td>0.706190</td>\n",
       "      <td>0.572587</td>\n",
       "      <td>16186</td>\n",
       "      <td>9268.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>lemma_no_POS</td>\n",
       "      <td>0:03:37</td>\n",
       "      <td>0.704077</td>\n",
       "      <td>0.566795</td>\n",
       "      <td>16186</td>\n",
       "      <td>9174.0</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>lemma_spacy_pipe_no_POS</td>\n",
       "      <td>0:12:38</td>\n",
       "      <td>0.696325</td>\n",
       "      <td>0.555985</td>\n",
       "      <td>16186</td>\n",
       "      <td>8999.0</td>\n",
       "      <td>369.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                        col time lemmatize        f1       tpr  \\\n",
       "0      2       lemma_spacy_with_POS        1:03:29  0.710763  0.578764   \n",
       "1      3  lemma_spacy_pipe_with_POS        0:23:22  0.710763  0.578764   \n",
       "2      1             lemma_with_POS        0:29:17  0.706190  0.572587   \n",
       "3      0               lemma_no_POS        0:03:37  0.704077  0.566795   \n",
       "4      4    lemma_spacy_pipe_no_POS        0:12:38  0.696325  0.555985   \n",
       "\n",
       "   count_toxic  detection_count_toxic  diff_with_best  \n",
       "0        16186                 9368.0             0.0  \n",
       "1        16186                 9368.0             0.0  \n",
       "2        16186                 9268.0           100.0  \n",
       "3        16186                 9174.0           194.0  \n",
       "4        16186                 8999.0           369.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l_cols = l_cols = ['lemma_no_POS', 'lemma_with_POS', 'lemma_spacy_with_POS', 'lemma_spacy_pipe_with_POS', \n",
    "                   'lemma_spacy_pipe_no_POS']\n",
    "\n",
    "l_f1 = []\n",
    "l_tpr = []\n",
    "\n",
    "if sklearn.__version__.split('.')[0] == '1':\n",
    "    stop_words = list(stop_words)\n",
    "\n",
    "for col in l_cols:\n",
    "    print('Column:', col)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[col], df['toxic'], test_size=.2, stratify=df['toxic'], \n",
    "                                                        random_state=RANDOM_STATE)\n",
    "\n",
    "    x_train2, x_valid, y_train2, y_valid = train_test_split(x_train, y_train, test_size = 0.2, \n",
    "                                                      stratify=y_train, random_state=RANDOM_STATE)\n",
    "\n",
    "    est = LogisticRegression(random_state=RANDOM_STATE, verbose=0)\n",
    "    tfidfv = TfidfVectorizer(stop_words=stop_words)\n",
    "    lr_lemma = Pipeline([('tfidfv', tfidfv),('est',est)])\n",
    "\n",
    "    lr_lemma.fit(x_train2, y_train2)\n",
    "\n",
    "    predict = lr_lemma.predict(x_valid)\n",
    "    l_f1.append(f1_score(y_valid, predict))\n",
    "    l_tpr.append(recall_score(y_valid, predict))\n",
    "\n",
    "count_toxic = df[df['toxic'] == 1].shape[0]\n",
    "df_test_lemm = pd.DataFrame({'col': l_cols, \n",
    "                             'time lemmatize': ['0:03:37', '0:29:17', '1:03:29', '0:23:22', '0:12:38'], \n",
    "                             'f1': l_f1, 'tpr': l_tpr})\n",
    "df_test_lemm = df_test_lemm.sort_values(by='f1', ascending=False).reset_index()\n",
    "\n",
    "\n",
    "df_test_lemm = df_test_lemm.sort_values(by='f1', ascending=False).reset_index(drop=True)\n",
    "df_test_lemm['count_toxic'] = count_toxic\n",
    "df_test_lemm['detection_count_toxic'] = round(df_test_lemm['count_toxic'] * df_test_lemm['tpr'], 0)\n",
    "df_test_lemm['diff_with_best'] = 0\n",
    "for i in range(1, len(df_test_lemm)):\n",
    "    df_test_lemm.loc[i, 'diff_with_best'] = df_test_lemm.loc[0, 'detection_count_toxic'] - df_test_lemm.loc[i, 'detection_count_toxic']\n",
    "display(df_test_lemm)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница в цифрах настолько незначительная, что использование `nltk без POS` является лучшим вариантом  учитывать возможность переобучения модели в будущем на новой порции данных,  а значит новой подготовки данных. В дальнейшем будем использовать эту колонку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные готовые, векторизовать применяя TF-IDF, будем уже на тренировочной выборке.\n",
    "\n",
    "### Вывод по разделу\n",
    "\n",
    "Что сделано: \n",
    "- загрузили файл, удалили лишнюю колонку\n",
    "- проверили пропуски - их нет, целевой признак определяет 2 класса: 0 и 1.\n",
    "- выявили дисбаланс - объектов класса, которые мы должны научится определять \"1\" намного меньше чем \"0\".\n",
    "- токенезировали/лемматизировали наш датасет ( это не является утечкой/протечкой данных) используя разные подходы. Лучший результат по совокупности показала `nltk` без POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение\n",
    "\n",
    "Разделим наш датасет на тренировочную и тестовую выборки, закодируем векторное представление текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127433,)\n"
     ]
    }
   ],
   "source": [
    "df_res = {}  # для хранения результатов обучения\n",
    "\n",
    "RANDOM_STATE = 100\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['lemma_no_POS'], df['toxic'], test_size = 0.2, \n",
    "                                                    random_state=RANDOM_STATE, stratify=df['toxic'])\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас уже указано значение целевой метрики: F1 >=  0.75. Поверим, может dummy модель позволит нам достичь этой метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1 score DummyClassifier = 0.0967392984082831'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_model = DummyClassifier(strategy='stratified', random_state=RANDOM_STATE)\n",
    "dummy_model.fit(x_train, y_train)\n",
    "dummy_predict = dummy_model.predict(x_train)\n",
    "\n",
    "f1 = f1_score(y_train, dummy_predict)\n",
    "display(f'F1 score DummyClassifier = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DummyClassifier нас не устраивает, так же мы не будем с ним сравнивать другие модели, т.к. целевое значение метрики не достигнуто.\n",
    "\n",
    "Для повышения качества модели будем использовать подбор гиперпараметров с применением cross-validation, для устранения утечки информации из тренировочного этапа в валидационный будем использовать конвейер для векторизации текста-обучения-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_parameters = {\n",
    "    'tfidf__max_df': [.25, .5, 0.75],\n",
    "    'logreg__C': [1, 5, 10, 20, 30, 35, 40]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Без балансировки"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('logreg', LogisticRegression(random_state=RANDOM_STATE))])\n",
    "\n",
    "lr_default = HalvingGridSearchCV(pipeline, logreg_parameters, cv=3, n_jobs=-1, scoring='f1', verbose=0)\n",
    "lr_default.fit(x_train, y_train)\n",
    "\n",
    "print(f'F1-score LogisticRegression: {lr_default.best_score_}')\n",
    "print(f'Лучшая модель -> LogisticRegression: {lr_default.best_params_}')\n",
    "\n",
    "df_res['LogisticRegression'] = {'F1': lr_default.best_score_, 'best_params_': lr_default.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балансировка с увеличением классов"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pipeline_oversampling = imbPipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('oversampling', RandomOverSampler(random_state=RANDOM_STATE)),\n",
    "    ('logreg', LogisticRegression(random_state=RANDOM_STATE))])\n",
    "\n",
    "\n",
    "lr_oversampling = HalvingGridSearchCV(pipeline_oversampling, logreg_parameters, cv=3, n_jobs=-1, scoring='f1', \n",
    "                                      verbose=0, random_state=RANDOM_STATE)\n",
    "lr_oversampling.fit(x_train, y_train)\n",
    "\n",
    "print(f'F1-score RandomOverSampler->LogisticRegression: {lr_oversampling.best_score_}')\n",
    "print(f'Лучшая модель RandomOverSampler->LogisticRegression: {lr_oversampling.best_params_}')\n",
    "\n",
    "df_res['RandomOverSampler->LogisticRegression'] = {'F1': lr_oversampling.best_score_, \n",
    "                                                   'best_params_': lr_oversampling.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балансировка с уменьшением классов"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pipeline_oversampling = imbPipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('oversampling', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('logreg', LogisticRegression(random_state=RANDOM_STATE))])\n",
    "\n",
    "lr_undersampling = HalvingGridSearchCV(pipeline_oversampling, logreg_parameters, cv=3, n_jobs=-1, scoring='f1', \n",
    "                                       verbose=0, random_state=RANDOM_STATE)\n",
    "lr_undersampling.fit(x_train, y_train)\n",
    "\n",
    "print(f'F1-score RandomUnderSampler->LogisticRegression: {lr_undersampling.best_score_}')\n",
    "print(f'Лучшая модель RandomUnderSampler->LogisticRegression: {lr_undersampling.best_params_}')\n",
    "\n",
    "df_res['RandomUnderSampler->LogisticRegression'] = {'F1': lr_undersampling.best_score_, \n",
    "                                                    'best_params_': lr_undersampling.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_parameters = {\n",
    "    'tfidf__max_df': [.5, 0.75],\n",
    "    'lgbm__n_estimators': [400, 600],\n",
    "    'lgbm__max_depth':  [25, 31],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Без балансировки"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('lgbm', LGBMClassifier(force_row_wise=True, random_state=RANDOM_STATE, verbose=0))])\n",
    "lgbmc_default = HalvingGridSearchCV(estimator=pipeline_lgbm, param_grid=lgbm_parameters, n_jobs=-1, random_state=RANDOM_STATE, \n",
    "                                  return_train_score=True, scoring='f1', verbose=1, refit=True)\n",
    "lgbmc_default.fit(x_train, y_train)\n",
    "\n",
    "print(f'F1-score LGBMClassifier: {lgbmc_default.best_score_}')\n",
    "print(f'Лучшая модель LGBMClassifier: {lgbmc_default.best_params_}')\n",
    "df_res['LGBMClassifier'] = {'F1': lgbmc_default.best_score_, 'best_params_': lgbmc_default.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is_unbalance=True"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pipeline_lgbm_unbal = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('lgbm', LGBMClassifier(is_unbalance=True, force_row_wise=True, random_state=RANDOM_STATE, verbose=0))])\n",
    "lgbmc_is_unbalance = HalvingGridSearchCV(estimator=pipeline_lgbm_unbal, param_grid=lgbm_parameters, n_jobs=-1, random_state=RANDOM_STATE, \n",
    "                                  return_train_score=True, scoring='f1', verbose=1, refit=True)\n",
    "lgbmc_is_unbalance.fit(x_train, y_train)\n",
    "\n",
    "print(f'F1-score LGBMClassifier(is_unbalance=True): {lgbmc_is_unbalance.best_score_}')\n",
    "print(f'Лучшая модель LGBMClassifier(is_unbalance=True): {lgbmc_is_unbalance.best_params_}')\n",
    "df_res['LGBMClassifier(is_unbalance=True)'] = {'F1': lgbmc_is_unbalance.best_score_, 'best_params_': lgbmc_is_unbalance.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балансировка с увеличением классов"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pipeline_lgbm_ov = imbPipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('oversampling', RandomOverSampler(random_state=RANDOM_STATE)),\n",
    "    ('lgbm', LGBMClassifier(force_row_wise=True, random_state=RANDOM_STATE, verbose=1))])\n",
    "lgbm_oversampling = HalvingGridSearchCV(estimator=pipeline_lgbm_ov, param_grid=lgbm_parameters, n_jobs=-1, random_state=RANDOM_STATE, \n",
    "                                       return_train_score=True, scoring='f1', verbose=1, refit=True)\n",
    "lgbm_oversampling.fit(x_train, y_train)\n",
    "\n",
    "print(f'F1-score RandomOverSampler->LGBMClassifier: {lgbm_oversampling.best_score_}')\n",
    "print(f'Лучшая модель RandomOverSampler->LGBMClassifier: {lgbm_oversampling.best_params_}')\n",
    "\n",
    "df_res['RandomOverSampler->LGBMClassifier'] = {'F1': lgbm_oversampling.best_score_, 'best_params_': lgbm_oversampling.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балансировка с уменьшением классов"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pipeline_lgbm_un = imbPipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('undersampling', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "    ('lgbm', LGBMClassifier(force_row_wise=True, random_state=RANDOM_STATE, verbose=1))])\n",
    "lgbm_undersampling = HalvingGridSearchCV(estimator=pipeline_lgbm_un, param_grid=lgbm_parameters, n_jobs=-1, random_state=RANDOM_STATE, \n",
    "                                        return_train_score=True, scoring='f1', verbose=1, refit=True)\n",
    "lgbm_undersampling.fit(x_train, y_train)\n",
    "\n",
    "print(f'F1-score RandomUnderSampler->LGBMClassifier: {lgbm_undersampling.best_score_}')\n",
    "print(f'Лучшая модель RandomUnderSampler->LGBMClassifier: {lgbm_undersampling.best_params_}')\n",
    "\n",
    "df_res['RandomUnderSampler->LGBMClassifier'] = {'F1': lgbm_undersampling.best_score_, 'best_params_': lgbm_undersampling.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результаты обучения:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "l_f1  = []\n",
    "l_params = []\n",
    "l_model_name = list(df_res.keys())\n",
    "for d in df_res.values():\n",
    "    l_f1.append(d['F1'])\n",
    "    l_params.append(d['best_params_'])\n",
    "pd.DataFrame({'model': l_model_name, 'f1': l_f1, 'best_params': l_params}).sort_values(by='f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.781251</td>\n",
       "      <td>{'lgbm__max_depth': 25, 'lgbm__n_estimators': 400, 'tfidf__max_df': 0.75}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomOverSampler-&gt;LGBMClassifier</td>\n",
       "      <td>0.777189</td>\n",
       "      <td>{'lgbm__max_depth': 25, 'lgbm__n_estimators': 600, 'tfidf__max_df': 0.75}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.772295</td>\n",
       "      <td>{'logreg__C': 30, 'tfidf__max_df': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LGBMClassifier(is_unbalance=True)</td>\n",
       "      <td>0.771733</td>\n",
       "      <td>{'lgbm__max_depth': 25, 'lgbm__n_estimators': 600, 'tfidf__max_df': 0.75}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomOverSampler-&gt;LogisticRegression</td>\n",
       "      <td>0.762199</td>\n",
       "      <td>{'logreg__C': 35, 'tfidf__max_df': 0.75}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomUnderSampler-&gt;LGBMClassifier</td>\n",
       "      <td>0.699863</td>\n",
       "      <td>{'lgbm__max_depth': 31, 'lgbm__n_estimators': 400, 'tfidf__max_df': 0.75}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomUnderSampler-&gt;LogisticRegression</td>\n",
       "      <td>0.688863</td>\n",
       "      <td>{'logreg__C': 5, 'tfidf__max_df': 0.5}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    model        f1  \\\n",
       "3                          LGBMClassifier  0.781251   \n",
       "5       RandomOverSampler->LGBMClassifier  0.777189   \n",
       "0                      LogisticRegression  0.772295   \n",
       "4       LGBMClassifier(is_unbalance=True)  0.771733   \n",
       "1   RandomOverSampler->LogisticRegression  0.762199   \n",
       "6      RandomUnderSampler->LGBMClassifier  0.699863   \n",
       "2  RandomUnderSampler->LogisticRegression  0.688863   \n",
       "\n",
       "                                                                 best_params  \n",
       "3  {'lgbm__max_depth': 25, 'lgbm__n_estimators': 400, 'tfidf__max_df': 0.75}  \n",
       "5  {'lgbm__max_depth': 25, 'lgbm__n_estimators': 600, 'tfidf__max_df': 0.75}  \n",
       "0                                    {'logreg__C': 30, 'tfidf__max_df': 0.5}  \n",
       "4  {'lgbm__max_depth': 25, 'lgbm__n_estimators': 600, 'tfidf__max_df': 0.75}  \n",
       "1                                   {'logreg__C': 35, 'tfidf__max_df': 0.75}  \n",
       "6  {'lgbm__max_depth': 31, 'lgbm__n_estimators': 400, 'tfidf__max_df': 0.75}  \n",
       "2                                     {'logreg__C': 5, 'tfidf__max_df': 0.5}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_temp = {'model': {0: 'LogisticRegression',\n",
    "  1: 'RandomOverSampler->LogisticRegression',\n",
    "  2: 'RandomUnderSampler->LogisticRegression',\n",
    "  3: 'LGBMClassifier',\n",
    "  4: 'LGBMClassifier(is_unbalance=True)',\n",
    "  5: 'RandomOverSampler->LGBMClassifier',\n",
    "  6: 'RandomUnderSampler->LGBMClassifier'},\n",
    " 'f1': {0: 0.7722946206292861,\n",
    "  1: 0.7621987840121163,\n",
    "  2: 0.6888629372620342,\n",
    "  3: 0.7812506222406077,\n",
    "  4: 0.7717331309580369,\n",
    "  5: 0.7771893889354047,\n",
    "  6: 0.6998634282885164},\n",
    " 'best_params': {0: {'logreg__C': 30, 'tfidf__max_df': 0.5},\n",
    "  1: {'logreg__C': 35, 'tfidf__max_df': 0.75},\n",
    "  2: {'logreg__C': 5, 'tfidf__max_df': 0.5},\n",
    "  3: {'lgbm__max_depth': 25, 'lgbm__n_estimators': 400, 'tfidf__max_df': 0.75},\n",
    "  4: {'lgbm__max_depth': 25, 'lgbm__n_estimators': 600, 'tfidf__max_df': 0.75},\n",
    "  5: {'lgbm__max_depth': 25, 'lgbm__n_estimators': 600, 'tfidf__max_df': 0.75},\n",
    "  6: {'lgbm__max_depth': 31,\n",
    "   'lgbm__n_estimators': 400,\n",
    "   'tfidf__max_df': 0.75}}}\n",
    "\n",
    "with pd.option_context('max_colwidth', None):\n",
    "    display(pd.DataFrame.from_dict(d_temp).sort_values(by='f1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод  по разделу\n",
    "\n",
    "Модель LogisticRegression была лучше, пока кол-во деревьев в  LGBMClassifier было меньше 400. В наших данных дисбаланс классов. Как пишет автор -  https://alexanderdyakonov.wordpress.com/2021/05/27/imbalance/ -  при использование в качестве  функции ошибки LogLoss, то балансировка не нужна: \"...Если используется LogLoss, то каких-то «танцев с перебалансировкой» делать не только не нужно, но и недопустимо, поскольку это делает решение (алгоритм) неоткалиброванным. ...\". Классы, которые мы использовали по дефолту используют как раз LogLoss. Но народня мудрость гласит \"доверяй, но проверяй\". Проверили встроенными возможностями, а так же с помощью ibmlearn - результат отрицательный, т.е. ни один из способов не был лучше варианта \"без борьбы с дисбалансом\".\n",
    "\n",
    "Лучшая модель: LGBMClassifier с параметрами {'lgbm__max_depth': 25, 'lgbm__n_estimators': 400, 'tfidf__max_df': 0.75} достигла 'f1' = 0.781251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1 score = 0.771532973725422'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer(max_df=0.75, stop_words=stop_words)\n",
    "est = LGBMClassifier(random_state=RANDOM_STATE, max_depth=25, n_estimators=400)\n",
    "model = Pipeline([('tfidfv', tfidfv),('est',est)])\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "f1 = f1_score(y_test, y_predict)\n",
    "display(f'F1 score = {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.966\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "proba = model.predict_proba(x_test)\n",
    "proba = proba[:, 1]\n",
    "auc = roc_auc_score(y_test, proba)\n",
    "print('AUC: %.3f' % auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, proba)\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.title('ROC curve best model')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель работы: разработка модели для оценки \"токсичности\" текстов. Целевая метрика качества: `F1` не меньше 0.75. \n",
    "    \n",
    "Датасет содержит ~160 тыс. размеченных текстов, из-них примерено ~10% - \"токсичных\".\n",
    "\n",
    "Для борьбы с дисбалансом, воспользовались встроенными возможностями, а так же с `ibmlearn`\n",
    "\n",
    "Для токенизации и лемматизации текстов применялись библиотеки `nltk` и `spcy` с учетом без слова в предложении, для определения части речи. \n",
    "Самый быстрый вариант `nltk without POS`. Для векторного представления текстов использовался класс `TfidfVectorizer`.\n",
    "\n",
    "В обучении использовался стандартными`Pipeline` и `Pipeline` из `imblearn`, `HalvingGridSearchCV` для кросс-валидации и подбора гиперпараметров.\n",
    "\n",
    "Обучили модели на основе классов:\n",
    "- LogisticRegression\n",
    "- LGBMClassifier\n",
    "\n",
    "Модели на основе деревьев обучались очень долго. \n",
    "\n",
    "Победила модель `LGBMClassifier` без применения средств борьбы с дисбалансом, F1-score:\n",
    "- на обучении 0.781251\n",
    "- на тестирование 0.77153"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1884,
    "start_time": "2023-05-21T19:36:04.712Z"
   },
   {
    "duration": 2,
    "start_time": "2023-05-21T19:36:06.598Z"
   },
   {
    "duration": 3602,
    "start_time": "2023-05-21T19:36:06.602Z"
   },
   {
    "duration": 292,
    "start_time": "2023-05-21T19:36:10.207Z"
   },
   {
    "duration": 12,
    "start_time": "2023-05-21T19:36:10.501Z"
   },
   {
    "duration": 21,
    "start_time": "2023-05-21T19:36:10.515Z"
   },
   {
    "duration": 34,
    "start_time": "2023-05-21T19:36:10.538Z"
   },
   {
    "duration": 40,
    "start_time": "2023-05-21T19:36:10.574Z"
   },
   {
    "duration": 979,
    "start_time": "2023-05-21T19:36:10.616Z"
   },
   {
    "duration": 203,
    "start_time": "2023-05-21T19:36:11.597Z"
   },
   {
    "duration": 813,
    "start_time": "2023-05-21T19:36:11.801Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-21T19:36:12.616Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-21T19:36:12.617Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-21T19:36:12.618Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-21T19:36:12.619Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-21T19:36:12.620Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-21T19:36:12.621Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-21T19:36:12.622Z"
   },
   {
    "duration": 7,
    "start_time": "2023-05-21T19:38:15.565Z"
   },
   {
    "duration": 467,
    "start_time": "2023-05-21T19:38:32.878Z"
   },
   {
    "duration": 518,
    "start_time": "2023-05-21T19:38:37.619Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-21T19:38:55.387Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-21T19:39:25.670Z"
   },
   {
    "duration": 37,
    "start_time": "2023-05-21T19:39:54.681Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-21T19:40:19.177Z"
   },
   {
    "duration": 1704,
    "start_time": "2023-05-21T19:40:34.901Z"
   },
   {
    "duration": 3,
    "start_time": "2023-05-21T19:40:36.607Z"
   },
   {
    "duration": 806,
    "start_time": "2023-05-21T19:40:36.612Z"
   },
   {
    "duration": 300,
    "start_time": "2023-05-21T19:40:37.421Z"
   },
   {
    "duration": 14,
    "start_time": "2023-05-21T19:40:37.724Z"
   },
   {
    "duration": 34,
    "start_time": "2023-05-21T19:40:37.741Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-21T19:40:37.777Z"
   },
   {
    "duration": 76,
    "start_time": "2023-05-21T19:40:37.792Z"
   },
   {
    "duration": 226,
    "start_time": "2023-05-21T19:40:37.870Z"
   },
   {
    "duration": 459,
    "start_time": "2023-05-21T19:40:38.099Z"
   },
   {
    "duration": 12,
    "start_time": "2023-05-21T19:40:38.560Z"
   },
   {
    "duration": 21,
    "start_time": "2023-05-21T19:40:38.574Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-21T19:40:38.598Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-21T19:40:38.604Z"
   },
   {
    "duration": 22,
    "start_time": "2023-05-21T19:40:38.611Z"
   },
   {
    "duration": 31,
    "start_time": "2023-05-21T19:40:38.635Z"
   },
   {
    "duration": 30522,
    "start_time": "2023-05-21T19:40:38.668Z"
   },
   {
    "duration": 465,
    "start_time": "2023-05-21T19:41:09.193Z"
   },
   {
    "duration": 1762,
    "start_time": "2023-05-21T19:41:28.913Z"
   },
   {
    "duration": 3,
    "start_time": "2023-05-21T19:41:30.682Z"
   },
   {
    "duration": 874,
    "start_time": "2023-05-21T19:41:30.687Z"
   },
   {
    "duration": 363,
    "start_time": "2023-05-21T19:41:31.564Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-21T19:41:31.930Z"
   },
   {
    "duration": 28,
    "start_time": "2023-05-21T19:41:31.963Z"
   },
   {
    "duration": 66,
    "start_time": "2023-05-21T19:41:31.994Z"
   },
   {
    "duration": 49,
    "start_time": "2023-05-21T19:41:32.066Z"
   },
   {
    "duration": 289,
    "start_time": "2023-05-21T19:41:32.117Z"
   },
   {
    "duration": 499,
    "start_time": "2023-05-21T19:41:32.408Z"
   },
   {
    "duration": 11,
    "start_time": "2023-05-21T19:41:32.909Z"
   },
   {
    "duration": 37,
    "start_time": "2023-05-21T19:41:32.922Z"
   },
   {
    "duration": 21,
    "start_time": "2023-05-21T19:41:32.961Z"
   },
   {
    "duration": 19,
    "start_time": "2023-05-21T19:41:32.986Z"
   },
   {
    "duration": 29,
    "start_time": "2023-05-21T19:41:33.006Z"
   },
   {
    "duration": 33,
    "start_time": "2023-05-21T19:41:33.038Z"
   },
   {
    "duration": 30571,
    "start_time": "2023-05-21T19:41:33.073Z"
   },
   {
    "duration": 480,
    "start_time": "2023-05-21T19:42:03.646Z"
   },
   {
    "duration": 1892,
    "start_time": "2023-05-28T14:31:23.301Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.196Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.198Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.200Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.201Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.203Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.204Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.206Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.207Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.209Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.210Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.212Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.214Z"
   },
   {
    "duration": 1,
    "start_time": "2023-05-28T14:31:25.215Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.217Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.218Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.219Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.220Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.222Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.223Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.224Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.225Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T14:31:25.226Z"
   },
   {
    "duration": 7023,
    "start_time": "2023-05-28T14:32:04.472Z"
   },
   {
    "duration": 4857,
    "start_time": "2023-05-28T14:33:28.746Z"
   },
   {
    "duration": 2387,
    "start_time": "2023-05-28T14:34:45.292Z"
   },
   {
    "duration": 4517,
    "start_time": "2023-05-28T14:34:47.683Z"
   },
   {
    "duration": 1479,
    "start_time": "2023-05-28T14:34:52.202Z"
   },
   {
    "duration": 3754,
    "start_time": "2023-05-28T14:34:53.683Z"
   },
   {
    "duration": 317,
    "start_time": "2023-05-28T14:34:57.440Z"
   },
   {
    "duration": 29,
    "start_time": "2023-05-28T14:34:57.759Z"
   },
   {
    "duration": 19,
    "start_time": "2023-05-28T14:34:57.792Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-28T14:34:57.813Z"
   },
   {
    "duration": 7194962,
    "start_time": "2023-05-28T14:34:57.824Z"
   },
   {
    "duration": 18,
    "start_time": "2023-05-28T16:34:52.788Z"
   },
   {
    "duration": 9554,
    "start_time": "2023-05-28T16:34:52.807Z"
   },
   {
    "duration": 970,
    "start_time": "2023-05-28T16:35:02.364Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.336Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.338Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.340Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.341Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.343Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.344Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.345Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.347Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.349Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.350Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.351Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:35:03.352Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-28T16:40:53.452Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-28T16:42:12.555Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-28T16:42:18.335Z"
   },
   {
    "duration": 99083,
    "start_time": "2023-05-28T16:42:31.320Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-28T16:48:30.498Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-28T16:49:43.558Z"
   },
   {
    "duration": 24003,
    "start_time": "2023-05-28T16:49:51.576Z"
   },
   {
    "duration": 7958,
    "start_time": "2023-05-28T16:50:21.795Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-28T16:50:34.272Z"
   },
   {
    "duration": 193305,
    "start_time": "2023-05-28T16:50:37.339Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T16:53:50.646Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-28T16:54:04.154Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-28T16:54:43.086Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-28T16:55:10.877Z"
   },
   {
    "duration": 21099,
    "start_time": "2023-05-28T16:55:23.071Z"
   },
   {
    "duration": 10699,
    "start_time": "2023-05-28T16:55:48.315Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-28T16:56:37.748Z"
   },
   {
    "duration": 225881,
    "start_time": "2023-05-28T16:56:52.655Z"
   },
   {
    "duration": 3541,
    "start_time": "2023-05-28T17:05:58.610Z"
   },
   {
    "duration": 17331,
    "start_time": "2023-05-28T17:06:02.153Z"
   },
   {
    "duration": 1221,
    "start_time": "2023-05-28T17:06:19.490Z"
   },
   {
    "duration": 3704,
    "start_time": "2023-05-28T17:06:20.715Z"
   },
   {
    "duration": 304,
    "start_time": "2023-05-28T17:06:24.421Z"
   },
   {
    "duration": 16,
    "start_time": "2023-05-28T17:06:24.727Z"
   },
   {
    "duration": 34,
    "start_time": "2023-05-28T17:06:24.744Z"
   },
   {
    "duration": 14,
    "start_time": "2023-05-28T17:06:24.784Z"
   },
   {
    "duration": 21,
    "start_time": "2023-05-28T17:06:24.800Z"
   },
   {
    "duration": 8428,
    "start_time": "2023-05-28T17:06:24.824Z"
   },
   {
    "duration": 250387,
    "start_time": "2023-05-28T17:06:33.256Z"
   },
   {
    "duration": 131,
    "start_time": "2023-05-28T17:10:43.645Z"
   },
   {
    "duration": 71,
    "start_time": "2023-05-28T17:10:43.778Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-28T17:10:43.851Z"
   },
   {
    "duration": 151632,
    "start_time": "2023-05-28T17:10:43.874Z"
   },
   {
    "duration": 25440,
    "start_time": "2023-05-28T17:13:15.509Z"
   },
   {
    "duration": 1671,
    "start_time": "2023-05-28T17:13:40.951Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-28T17:13:42.624Z"
   },
   {
    "duration": 9,
    "start_time": "2023-05-28T17:13:42.629Z"
   },
   {
    "duration": 53,
    "start_time": "2023-05-28T17:13:42.639Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T17:13:42.694Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T17:13:42.696Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-28T17:13:42.697Z"
   },
   {
    "duration": 2279,
    "start_time": "2023-05-28T17:14:08.691Z"
   },
   {
    "duration": 4659,
    "start_time": "2023-05-28T17:14:10.973Z"
   },
   {
    "duration": 797,
    "start_time": "2023-05-28T17:14:15.634Z"
   },
   {
    "duration": 863,
    "start_time": "2023-05-28T17:14:16.433Z"
   },
   {
    "duration": 276,
    "start_time": "2023-05-28T17:14:17.299Z"
   },
   {
    "duration": 18,
    "start_time": "2023-05-28T17:14:17.577Z"
   },
   {
    "duration": 15,
    "start_time": "2023-05-28T17:14:17.596Z"
   },
   {
    "duration": 10,
    "start_time": "2023-05-28T17:14:17.613Z"
   },
   {
    "duration": 53,
    "start_time": "2023-05-28T17:14:17.624Z"
   },
   {
    "duration": 8275,
    "start_time": "2023-05-28T17:14:17.679Z"
   },
   {
    "duration": 251861,
    "start_time": "2023-05-28T17:14:25.956Z"
   },
   {
    "duration": 125,
    "start_time": "2023-05-28T17:18:37.820Z"
   },
   {
    "duration": 89,
    "start_time": "2023-05-28T17:18:37.947Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-28T17:18:38.039Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-28T17:18:38.045Z"
   },
   {
    "duration": 39,
    "start_time": "2023-05-28T17:18:38.053Z"
   },
   {
    "duration": 471147,
    "start_time": "2023-05-28T17:18:38.094Z"
   },
   {
    "duration": 12512,
    "start_time": "2023-05-28T17:26:29.242Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
